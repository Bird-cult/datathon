{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "#os.listdir(\".\")\n",
    "#credit = pd.read_csv(\"Credit_DataSet.csv\")\n",
    "\n",
    "# TODO: READ THE PROCESSED CSV INSTEAD (FOR EXAMPLE WITH CONVERTED CURRENCY)!\n",
    "\n",
    "credit = pd.read_excel(\"./Credit/Credit_DataSet.xlsx\", sheetname=\"DATA\", skiprows=0)\n",
    "\n",
    "# Quickly select numeric columns\n",
    "\n",
    "to_keep = ['NBR_EMPLOYEES', 'COUNTRY_RISK_GROWTH_RATE',\n",
    "       'COUNTRY_RISK_DIVIDEND_YIELD', 'COUNTRY_RISK_PAYOUT_RATIO',\n",
    "       'VOLATILITY_30D', 'VOLATILITY_180D', 'PCT_CHG_1_YEAR', 'PCT_CHG_6_M',\n",
    "       'DIVIDEND_YIELD', 'MARKETCAP', 'TOTAL_ASSETS', 'TOTAL_LIABILITIES',\n",
    "       'CURRENT_ASSETS', 'EBIT', 'RETAINED_EARNINGS', 'SALES', 'SALES_GROWTH',\n",
    "       'INTEREST_EXPENSES']\n",
    "\n",
    "X = credit[ to_keep + [\"DEFAULT_PROB\"] ].dropna()\n",
    "\n",
    "Y = X[\"DEFAULT_PROB\"].values\n",
    "X = X[to_keep].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# READ PREPROCESSED DATA FROM R FOR NN\n",
    "X_df = pd.read_csv(\"some_X.csv\", usecols=['NBR_EMPLOYEES', 'EMPL_GROWTH',\n",
    "       'COUNTRY_RISK_GROWTH_RATE', 'COUNTRY_RISK_DIVIDEND_YIELD',\n",
    "       'COUNTRY_RISK_PAYOUT_RATIO', 'VOLATILITY_30D', 'VOLATILITY_180D',\n",
    "       'PCT_CHG_1_YEAR', 'PCT_CHG_6_M', 'DIVIDEND_YIELD', 'MARKETCAP',\n",
    "       'TOTAL_ASSETS', 'TOTAL_LIABILITIES', 'CURRENT_ASSETS', 'EBIT',\n",
    "       'RETAINED_EARNINGS', 'SALES', 'SALES_GROWTH', 'INTEREST_EXPENSES',\n",
    "        'WHISTLE_BLOWER_POLICY', 'ETHICS_POLICY', 'BRIBERY_POLICY'])\n",
    "Y_df = pd.read_csv(\"some_Y.csv\", usecols=[1])\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_likely(mlp, loss_fn, x_row, y_row):\n",
    "    candidates = [] # TODO: For every -1 in row in the specific columns, try 0 and 1\n",
    "    \n",
    "    bad_values = {'WHISTLE_BLOWER_POLICY': 0,\n",
    "                            'ETHICS_POLICY': 0,\n",
    "                            'BRIBERY_POLICY': 0}\n",
    "\n",
    "    possible_values_dict = {'WHISTLE_BLOWER_POLICY':[[1,0], [0,1]],\n",
    "                            'ETHICS_POLICY':[[1,0], [0,1]],\n",
    "                            'BRIBERY_POLICY':[[1,0], [0,1]]}\n",
    "    \n",
    "    # Make a dictionary for the candidate values that we will expand later\n",
    "    candidate_dict = {}\n",
    "    for k, v in x_row.iteritems():\n",
    "        candidate_dict[k] = []\n",
    "        \n",
    "        if (k in bad_values): # We have to encode for the neural network\n",
    "            if (v == bad_values[k]):\n",
    "                for i in possible_values_dict[k]:\n",
    "                    # Directly encode the bad_value\n",
    "                    candidate_dict[k].append(i)\n",
    "            else:\n",
    "                candidate_dict[k] = [possible_values_dict[k][int(v) - 1]] # HACK: IT'S BECAUSE THEY HAVE VALUES 1 AND 2\n",
    "        else:\n",
    "            # Just add the value\n",
    "            candidate_dict[k].append([v])\n",
    "\n",
    "    best_candidate = None\n",
    "    lowest_loss = 100000000000\n",
    "\n",
    "    # In the dictionary we have the different candidates\n",
    "    for candidate in product(*candidate_dict.values()):\n",
    "        input = Variable(torch.FloatTensor(np.array(list(chain(*list(candidate)))))).cuda()\n",
    "        # Forward the candidate through the network        \n",
    "        y_pred = mlp(input)\n",
    "        loss = loss_fn(y_pred, y_row)\n",
    "        if loss < lowest_loss:\n",
    "            best_candidate = input\n",
    "    return best_candidate\n",
    "    \n",
    "def get_E_step(mlp, loss_fn, X_df, Y_df):\n",
    "    \"\"\" Return a torch CUDA variable of X\"\"\"\n",
    "    df_list = []\n",
    "    #for x_row, y_row in zip(X_df.iterrows(), Y_df.iterrows()):\n",
    "    #    df_list.append(select_most_likely(mlp, loss_fn, x_row, y_row))\n",
    "        \n",
    "    for (k, j) in zip(X_df.iterrows(), Y_df.iterrows()):\n",
    "        (_, x) = k\n",
    "        (_, y) = j\n",
    "        df_list.append(select_most_likely(model, loss_fn, x, Variable(torch.FloatTensor(y.values)).cuda()))\n",
    "\n",
    "    return torch.stack(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for (k, j) in zip(X_df.iterrows(), Y_df.iterrows()):\n",
    "    (_, x) = k\n",
    "    (_, y) = j\n",
    "    df_list.append(select_most_likely(model, loss_fn, x, Variable(torch.FloatTensor(y.values)).cuda()))\n",
    "\n",
    "df = pd.DataFrame(df_list)\n",
    "#from itertools import * \n",
    "\n",
    "#select_most_likely(model, loss_fn, bla, 1)\n",
    "#select_most_likely(model, loss_fn, bla, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_M_step(mlp, loss_fn, X, Y, N_iter):\n",
    "    learning_rate = 1e-5\n",
    "    for t in range(N_iter):\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, Y)\n",
    "        if (t % (N_iter / 2) == 0): print(loss)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "    return mlp, loss\n",
    "\n",
    "def EM_algorithm(X_df, Y_df, N_hidden=10, N_iter=10000):\n",
    "    \n",
    "    X = Variable(torch.FloatTensor(X_df.values)).cuda()\n",
    "    Y = Variable(torch.FloatTensor(Y_df.values)).cuda() # Always stays the same\n",
    "\n",
    "    # Make an initial model\n",
    "    print(\"Making model\")\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(X.shape[1] + 3, N_hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(N_hidden, Y.shape[1]),\n",
    "        torch.nn.Sigmoid() # If it doesn't work, look here\n",
    "    )\n",
    "\n",
    "    model = model.cuda()\n",
    "    loss_fn = torch.nn.SoftMarginLoss(size_average=False)\n",
    "    \n",
    "    previous_loss = -50\n",
    "    # Init\n",
    "    for i in range(100):\n",
    "        # Get the expected data\n",
    "        print(\"Doing iteration {} E step\".format(i))\n",
    "        X = get_E_step(model, loss_fn, X_df, Y_df)\n",
    "        # TODO: Check if the predicted data is the same still?\n",
    "        # Train the model for some iterations\n",
    "        print(\"Doing iteration {} M step\".format(i))\n",
    "        model, loss = get_M_step(model, loss_fn, X, Y, N_iter)\n",
    "        if loss == previous_loss: \n",
    "            print(\"Loss not decreasing: exit\")\n",
    "            break\n",
    "        previous_loss = loss\n",
    "    return X, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model\n",
      "Doing iteration 0 E step\n",
      "Doing iteration 0 M step\n",
      "Variable containing:\n",
      " 2694.2021\n",
      "[torch.cuda.FloatTensor of size () (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 2694.2021\n",
      "[torch.cuda.FloatTensor of size () (GPU 0)]\n",
      "\n",
      "Doing iteration 1 E step\n",
      "Doing iteration 1 M step\n",
      "Variable containing:\n",
      " 2694.2021\n",
      "[torch.cuda.FloatTensor of size () (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 2694.2021\n",
      "[torch.cuda.FloatTensor of size () (GPU 0)]\n",
      "\n",
      "Loss not decreasing: exit\n"
     ]
    }
   ],
   "source": [
    "X_final, mlp = EM_algorithm(X_df, Y_df, N_hidden=100, N_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -0.2657  -0.0431  -4.9499  ...    0.0000   1.0000   0.0000\n",
       " -0.3394  -0.0584   0.2982  ...    0.0000   1.0000   0.0000\n",
       " -0.3244  -0.0405  -4.9499  ...    0.0000   1.0000   0.0000\n",
       "           ...               â‹±              ...            \n",
       " -0.3179  -0.0322   0.2982  ...    0.0000   1.0000   0.0000\n",
       " -0.3059  58.2679   0.2982  ...    0.0000   1.0000   0.0000\n",
       "  0.2978  -0.0363   1.2324  ...    0.0000   1.0000   0.0000\n",
       "[torch.cuda.FloatTensor of size (3933,25) (GPU 0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(df_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
